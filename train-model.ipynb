{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import các thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras_nlp.api.models import DebertaV3Preprocessor\n",
    "from keras_nlp.api.models import DebertaV3Classifier\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các tham số"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Tham khảo : https://www.kaggle.com/code/awsaf49/aes-2-0-kerasnlp-starter#%F0%9F%8D%BD%EF%B8%8F-%7C-Preprocessing\n",
    "# NOTE: đọc hết TODO\n",
    "\n",
    "# TODO: đổi path file csv\n",
    "data_path = \".csv\"\n",
    "# Sử dụng model DeBERTa-V3-Extra-Small-English\n",
    "model_preset = \"deberta_v3_extra_small_en\"\n",
    "max_epochs = 10\n",
    "batch_size = 8\n",
    "shufle = 1000\n",
    "lr = 0.0000003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các hàm chính để xử lý model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Label giá trị trị từ 1-6 (cột score) là dữ liệu categorical.\n",
    " Nhưng để làm bài này, label phải là dữ liệu ordinary\n",
    " Biến đổi label CATEGORICAL SANG ORDINARY bằng cách :\n",
    "\n",
    " Encode label thành một vector nhị phân có độ dài 6, bit i bằng 1 nghĩa là đã thỏa yêu cầu i\n",
    " Các giá trị trong vector CHO BIẾT XÁC SUẤT VỊ TRÍ i THỎA YÊU CẦU i.\n",
    " Từ đó có thể sử dụng CROSS-ENTROPY để sử dụng hàm lỗi so sánh sự tương đồng giữa 2 phân phối.\n",
    "\n",
    " Từ trên ta tạo tính ORDINARY như sau:\n",
    "\n",
    " Bài được chấm điểm 1 <= i <= 6 nghĩa là đã thỏa mãn yêu cầu i, VÀ CŨNG THỎA TẤT CẢ CÁC YÊU CẦU TRƯỚC ĐÓ (<= i),\n",
    " nên các bit trong vector nhãn sẽ = 1 ở các vị trí <= i, và 0 ở các vị trí còn lại.\n",
    " Cách làm này giúp có TÍNH SO SÁNH, vì một khi được điểm j >= i, thì các bit 1 của điểm i cũng = 1 ở trong j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ordinary(y):\n",
    "    n = len(y)\n",
    "    z = np.zeros((n, 6), \"float32\")\n",
    "    for i in range(n):\n",
    "        s = y[i]\n",
    "        z[i, :s] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: code tiền xử lý đầu vào"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tạo và cấu hình model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model đã được cấu hình và train sẵn, sau đó train để fine-tune model này phù hợp cho bài toán. </br>\n",
    "Như đã nói trên sử đầù ra là một vector độ dài 6, mỗi vị trí cho biết xác suất thỏa yêu cầu i.</br>\n",
    "Đầu ra của mạng sử dụng sigmoid cho ra giá trị 0->1, chuẩn hóa thành xác suất cho mỗi vị trí i.</br>\n",
    "Sử dụng hàm Binary Cross-Entropy, vì ta tính xác suất mỗi vị trí (mỗi vị trí đúng (1) đến sai (0)),\n",
    "sau đó keras.losses.BinaryCrossentropy tổng hợp Loss tại mỗi vị trí làm Loss chung\n",
    "Sử dụng Adam optimizer kết hợp SGD và momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DebertaV3_model(model_preset, manual_preprocess=False):\n",
    "    if not manual_preprocess:\n",
    "        debertaV3 = DebertaV3Classifier.from_preset(model_preset, num_classes=6)\n",
    "    else:\n",
    "        debertaV3 = DebertaV3Classifier.from_preset(\n",
    "            model_preset, preprocessor=None, num_classes=6\n",
    "        )\n",
    "    inputs = debertaV3.input\n",
    "    outputs = debertaV3(inputs)\n",
    "\n",
    "    prob_outputs = keras.layers.Activation(\"sigmoid\")(outputs)\n",
    "    model = keras.Model(inputs, prob_outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tạo dataset cho dữ liệu train và validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dữ liệu từ file csv và tạo tf.data.Dataset cho dữ liệu train và dữ liệu validation.</br>\n",
    "Dataset load dữ liệu theo lô (và cache để tiết kiệm bộ nhớ trong) và hoán vị dữ liệu sau mỗi vòng lặp cho THUẬT TOÁN SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    df = pd.read_csv(data_path)\n",
    "    X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(\n",
    "        df[\"full_text\"], df[\"score\"], test_size=0.2, stratify=df[\"score\"]\n",
    "    )\n",
    "    X_train = X_train_df.tolist()\n",
    "    X_val = X_val_df.tolist()\n",
    "    y_train = make_ordinary(y_train_df.tolist())\n",
    "    y_val = make_ordinary(y_val_df.tolist())\n",
    "\n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .cache()\n",
    "        .shuffle(shufle, seed=0)\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "    opt = tf.data.Options()\n",
    "    opt.experimental_deterministic = False\n",
    "    train_ds = train_ds.with_options(opt)\n",
    "\n",
    "    val_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val, y_val)).cache().batch(batch_size)\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huấn luyện model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = create_dataset()\n",
    "model = create_DebertaV3_model(model_preset)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "save_model = keras.callbacks.ModelCheckpoint(\n",
    "    \"model.h5\",  # HDF5\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[save_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thống kê lỗi và đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epoch_rows = range(1, len(training_loss) + 1)\n",
    "plt.plot(epoch_rows, training_loss, \"r--\")\n",
    "plt.plot(epoch_rows, validation_loss, \"b-\")\n",
    "plt.legend([\"Training Loss\", \"Validation Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.savefig(\"Loss_plot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
